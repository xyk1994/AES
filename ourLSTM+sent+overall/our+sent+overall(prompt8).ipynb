{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"our+sent+overall(prompt8).ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"Ksl3StCKHoWb","colab_type":"code","colab":{}},"source":["prompt_id = 8\n","model_name = \"our+sent+overall\"  \n","    # \"our\", \"our+sent\", \"our+overall\", \"our+sent+overall\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qfy1DEUuH1wE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":243},"outputId":"87da7eb0-9bba-4116-cef5-714829af3b96","executionInfo":{"status":"ok","timestamp":1578489629658,"user_tz":-540,"elapsed":2453,"user":{"displayName":"謝一寛","photoUrl":"","userId":"15265375714486849810"}}},"source":["import os\n","import pprint\n","import tensorflow as tf\n","\n","if 'COLAB_TPU_ADDR' not in os.environ:\n","  print('ERROR: Not connected to a TPU runtime')\n","else:\n","  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n","  print ('TPU address is', tpu_address)\n","\n","  with tf.Session(tpu_address) as session:\n","    devices = session.list_devices()\n","\n","  print('TPU devices:')\n","  pprint.pprint(devices)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["TPU address is grpc://10.58.121.18:8470\n","TPU devices:\n","[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 17479356826948551606),\n"," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 2337325760236256746),\n"," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 588217720173494498),\n"," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 10595991620463859374),\n"," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 13282477083993417532),\n"," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 16816213025472654850),\n"," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 5171917773097580616),\n"," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 8241118809373531552),\n"," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 399328435835160273),\n"," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 16765260380182858677),\n"," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 3747735776305685846)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_nWoE_biH70P","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":124},"outputId":"49867ef7-92ca-4481-bb44-bdeb309c130b","executionInfo":{"status":"ok","timestamp":1578489650083,"user_tz":-540,"elapsed":22856,"user":{"displayName":"謝一寛","photoUrl":"","userId":"15265375714486849810"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","path = \"/content/drive/My Drive/AES謝\"\n","os.chdir(path)\n","# os.listdir(path)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CC_R2_vlH97v","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"outputId":"61b5787e-e8b7-4b42-eb13-8ed0502a4b9b","executionInfo":{"status":"ok","timestamp":1578489651697,"user_tz":-540,"elapsed":24447,"user":{"displayName":"謝一寛","photoUrl":"","userId":"15265375714486849810"}}},"source":["import argparse\n","import json\n","import nltk\n","import keras\n","import tensorflow as tf\n","from keras.models import Model\n","from keras import layers\n","from keras.layers import Input, concatenate, Embedding, LSTM, Dense, Lambda\n","from keras.engine.topology import Layer\n","from keras.optimizers import Adam\n","from keras.callbacks import EarlyStopping\n","import re\n","import pandas as pd\n","import numpy as np\n","from nltk.corpus import stopwords\n","from nltk.tokenize import sent_tokenize\n","import keras.backend as K\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import cohen_kappa_score\n","from sklearn.preprocessing import StandardScaler\n","from keras.preprocessing.sequence import pad_sequences\n","import warnings\n","warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","# stopwords, punkt, "],"execution_count":4,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"cZhJhkRwICXN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"7494fb55-7504-48ed-bec5-d7b451d45012","executionInfo":{"status":"error","timestamp":1578490639804,"user_tz":-540,"elapsed":1012531,"user":{"displayName":"謝一寛","photoUrl":"","userId":"15265375714486849810"}}},"source":["# Mean Over Time\n","class TemporalMean(Layer):\n","  def __init__(self, **kwargs):\n","      self.supports_masking = True\n","      super(TemporalMean, self).__init__(**kwargs)\n","\n","  def compute_mask(self, input, input_mask=None):\n","      # do not pass the mask to the next layers\n","      return None\n","\n","  def call(self, x, mask=None):\n","      if mask is not None:\n","          # mask (batch, time)\n","          mask = K.cast(mask, K.floatx())\n","          # mask (batch, x_dim, time)\n","          mask = K.repeat(mask, x.shape[-1])\n","          # mask (batch, time, x_dim)\n","          mask = tf.transpose(mask, [0,2,1])\n","          x = x * mask\n","      return K.sum(x, axis=1) / K.sum(mask, axis=1)\n","\n","  def compute_output_shape(self, input_shape):\n","      # remove temporal dimension\n","      return (input_shape[0], input_shape[2])\n","\n","def essay_to_wordlist(essay_v, remove_stopwords):\n","    \"\"\"Remove the tagged labels and word tokenize the sentence.\"\"\"\n","    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n","    words = essay_v.lower().split()\n","    if remove_stopwords:\n","        stops = set(stopwords.words(\"english\"))\n","        words = [w for w in words if not w in stops]\n","    return (words)\n","\n","def get_clean_essays(Text):\n","    clean_essays = []\n","    for essay in Text:\n","        clean_essays.append(essay_to_wordlist(essay, remove_stopwords=False))\n","    return clean_essays\n","\n","def create_dict(text):\n","    print(\"create_dict\")\n","    word_index = {}\n","    word_index['xieyikuan'] = 0\n","    for sentence in text:\n","        for word in sentence:\n","            if word not in word_index:\n","                word_index[word] = len(word_index)\n","    word_index['unk'] = len(word_index)\n","    return word_index\n","\n","def essay_to_index(essays, word_index):\n","    Essays = []\n","    for essay in essays:\n","        sentence = []\n","        for sent in essay:\n","            words = []\n","            for word in sent:\n","                words.append(word_index[word])\n","                '''\n","                if word in word_index:\n","                    words.append(word_index[word])\n","                elif word not in word_index:\n","                    words.append(word_index['unk'])\n","                '''\n","            sentence.append(words)\n","        Essays.append(sentence)\n","    return Essays\n","\n","def get_embeddings_index():\n","    embeddings_index = {}\n","    f = open('glove.6B.50d.txt', 'r', encoding='UTF-8')\n","    for line in f:\n","        values = line.split()\n","        word = values[0]\n","        coefs = np.array(values[1:], dtype='float32')\n","        embeddings_index[word] = coefs\n","    f.close()\n","    return embeddings_index\n","\n","def embeddings_matrix(embeddings_index, word_index, embedding_dim):\n","    nb_words = len(word_index)\n","    embedding_matrix = np.zeros((nb_words + 1, embedding_dim))\n","    for word, i in word_index.items():\n","        if i > nb_words:\n","            continue\n","        embeddings_vector = embeddings_index.get(word)\n","        if embeddings_vector is not None:\n","            embedding_matrix[i] = embeddings_vector\n","        if embeddings_vector is None:\n","            embedding_matrix[i] = embeddings_index.get('unk')\n","    return embedding_matrix\n","\n","def get_pst_feature(essays):\n","    pst_feature = []\n","    for essay in essays:\n","        position = []\n","        i = 1\n","        for sent in essay:\n","            position.append(max(1 / i, 1 / (len(essay) - i + 1)))\n","            i += 1\n","        position = np.array(position).reshape(1, -1)\n","        pst_feature.append(position)\n","    return pst_feature\n","\n","def stack(tensor):\n","    Tensor = keras.backend.stack(tensor, axis=1)\n","    return Tensor\n","\n","# --------\n","csv_input = pd.read_csv(filepath_or_buffer=\"data_feature/set\"+str(prompt_id)+\"_feature(26).tsv\", delimiter=\"\\t\", engine=\"python\", error_bad_lines=False)\n","\n","Text = csv_input['essay']\n","Score = csv_input['domain1_score']\n","Label = csv_input['domain1_score']\n","\n","# --------\n","if prompt_id == 1:\n","    Score = (Score - 2) / 10        # set_1     范围 2-12\n","    max_sent_length, max_word_length = 40, 35\n","elif prompt_id == 2:\n","    Score = (Score - 1) / 5         # set_2     范围 1-6\n","    max_sent_length, max_word_length = 40, 40\n","elif prompt_id == 3:\n","    Score = Score / 3               # set_3     范围 0-3\n","    max_sent_length, max_word_length = 15, 40\n","elif prompt_id == 4:\n","    Score = Score / 3               # set_4     范围 0-3\n","    max_sent_length, max_word_length = 10, 50\n","elif prompt_id == 5:\n","    Score = Score / 4               # set_5     范围 0-4\n","    max_sent_length, max_word_length = 15, 40\n","elif prompt_id == 6:\n","    Score = Score / 4               # set_6     范围 0-4\n","    max_sent_length, max_word_length = 15, 40\n","elif prompt_id == 7:\n","    Score = Score / 30              # set_7     范围 0-30\n","    max_sent_length, max_word_length = 27, 35\n","elif prompt_id == 8:\n","    Score = Score / 60              # set_8     范围 0-60\n","    max_sent_length, max_word_length = 65, 37\n","\n","\n","# Get Overall Feature (25)\n","if model_name in [\"our+overall\", \"our+overall_mot\", \"our+sent+overall\", \"our+sent+overall_mot\", \"our+all\", \"our+all_mot\"]:\n","    overall_feature = csv_input[['word_count', 'avg_word_len', 'sentence_count', 'avg_sentence_len', 'lemma_count', 'noun_count', 'adj_count', 'adv_count', 'verb_count', 'cc_count',\n","                                 'syllable_count', 'flesch_reading_ease', 'smog_index', 'flesch_kincaid_grade', 'coleman_liau_index', 'automated_readability_index',\n","                                 'dale_chall_readability_score', 'difficult_words', 'linsear_write_formula',\n","                                 'gunning_fog', 'spelling_errors', 'exc_count', 'que_count', 'comma_count', 'stopwords_num']]\n","    # overall_feature = np.array(np.sqrt(overall_feature))\n","    overall_feature = np.array(overall_feature)\n","    num_feature = 25\n","    where_are_nan = np.isnan(overall_feature)\n","    where_are_inf = np.isinf(overall_feature)\n","    overall_feature[where_are_nan] = 0\n","    overall_feature[where_are_inf] = 0\n","    # standardized\n","    stdsc = StandardScaler()\n","    overall_feature = stdsc.fit_transform(overall_feature)\n","\n","# Get the Second Last Layer of BERT\n","if model_name in [\"our+bert\", \"our+bert_mot\", \"our+all\", \"our+all_mot\"]:\n","    bert = pd.read_csv(filepath_or_buffer=\"fine_tuning_out/essay_set_\"+str(prompt_id)+\"_output.tsv\", delimiter=\"\\t\", engine=\"python\", error_bad_lines=False)\n","    bert = np.array(bert)\n","    max_seq_length = len(bert[0])\n","\n","# 做字典\n","clean_essays = get_clean_essays(Text)\n","word_index = create_dict(clean_essays)\n","\n","# 做分句\n","text = np.array(Text)\n","a = []\n","for essay in text:\n","    a.append(sent_tokenize(essay))\n","texts = np.array(a)\n","\n","# 做分词\n","essays = []\n","for essay in texts:\n","    sentence = []\n","    for sents in essay:\n","        sent = essay_to_wordlist(sents, remove_stopwords=True)\n","        # sent = np.array(sent)\n","        sentence.append(sent)\n","    essays.append(sentence)\n","essays = np.array(essays)\n","\n","Essays = essay_to_index(essays, word_index)\n","Essays = np.array(Essays)\n","\n","# Get Sentence Feature\n","if model_name in [\"our+sent\", \"our+sent_mot\", \"our+sent+overall\", \"our+sent+overall_mot\", \"our+all\"\"our+all_mot\"]:\n","    f = open('sent_feature/set'+str(prompt_id)+'_sent.json', 'r+')\n","    str_json = f.read()\n","    temp = json.loads(str_json)\n","    essay_num = len(temp)\n","    sent_num = max_sent_length\n","    sent_features = []\n","    for i in range(len(temp)):\n","        essay = temp['essay'+str(i)]\n","        feature_sent2essay = []\n","        for j in range(sent_num):\n","            sent = essay['sent'+str(j)]\n","            feature_sent = []\n","            feature_sent.append(sent['word_count'])\n","            feature_sent.append(sent['avg_word_len'])\n","            feature_sent.append(sent['lemma_count'])\n","            feature_sent.append(sent['noun_count'])\n","            feature_sent.append(sent['adj_count'])\n","            feature_sent.append(sent['adv_count'])\n","            feature_sent.append(sent['verb_count'])\n","            feature_sent.append(sent['cc_count'])\n","            feature_sent.append(sent['syllable_count'])\n","            feature_sent.append(sent['flesch_reading_ease'])\n","            feature_sent.append(sent['flesch_kincaid_grade'])\n","            feature_sent.append(sent['coleman_liau_index'])\n","            feature_sent.append(sent['automated_readability_index'])\n","            feature_sent.append(sent['dale_chall_readability_score'])\n","            feature_sent.append(sent['difficult_words'])\n","            feature_sent.append(sent['linsear_write_formula'])\n","            feature_sent.append(sent['gunning_fog'])\n","            feature_sent.append(sent['spelling_errors'])\n","            feature_sent.append(sent['stopwords_num'])\n","            feature_sent.append(sent['comma_count'])\n","            feature_sent2essay.append(feature_sent)\n","        sent_features.append(feature_sent2essay)\n","    sent_features = np.array(sent_features)\n","\n","    pst_feature = get_pst_feature(essays)\n","    pad_pst_feature = []\n","    for feature in pst_feature:\n","        pad_feature = pad_sequences(feature, maxlen=max_sent_length, dtype='float32')\n","        pad_pst_feature.append(pad_feature)\n","    position_feature = np.array(pad_pst_feature)\n","    position_feature = np.array([mat.T for mat in position_feature])\n","\n","    sent_features = np.concatenate((position_feature, sent_features), axis=-1)\n","    num_sent_feature = 21\n","\n","    stdsc = StandardScaler()\n","    Sent_features = []\n","    for essay in sent_features:\n","        Sent_features.append(stdsc.fit_transform(essay))\n","    sent_features = np.array(Sent_features)\n","\n","# Padding Words and Sentences\n","essay_pad = []\n","for essay in Essays:\n","    pad = pad_sequences(essay, maxlen=max_word_length)\n","    essay_pad.append(pad)\n","essays_pad = []\n","for essay in essay_pad:\n","    essays = []\n","    for sent in essay:\n","        for word in sent:\n","            essays.append(word)\n","    essays_pad.append(essays)\n","essays_pad = np.array(essays_pad)\n","Padding_Length = max_word_length * max_sent_length\n","essays_pad = pad_sequences(essays_pad, maxlen=Padding_Length)\n","\n","# Get Embedding Matrix\n","embeddings_index = get_embeddings_index()\n","embeddings_matrix = embeddings_matrix(embeddings_index, word_index, embedding_dim=50)\n","\n","# 记录成绩\n","results = []\n","score_predict_list = []\n","sssscore = []\n","llllabel = []\n","newresults = []\n","cv = KFold(n_splits=5, shuffle=True, random_state=10)\n","count = 0\n","for train, test in cv.split(essays_pad):\n","    text_train, text_test = essays_pad[train], essays_pad[test]\n","    label_train, label_test = Score.iloc[train], Score.iloc[test]\n","    score_train, score_test = Label.iloc[train], Label.iloc[test]\n","    if model_name in [\"our+overall\", \"our+overall_mot\", \"our+sent+overall\", \"our+sent+overall_mot\", \"our+all\", \"our+all_mot\"]:\n","        overall_train, overall_test = overall_feature[train], overall_feature[test]\n","    if model_name in [\"our+sent\", \"our+sent_mot\", \"our+sent+overall\", \"our+sent+overall_mot\", \"our+all\"\"our+all_mot\"]:\n","        sent_train, sent_test = sent_features[train], sent_features[test]\n","    if model_name in [\"our+bert\", \"our+bert_mot\", \"our+all\", \"our+all_mot\"]:\n","        bert_train, bert_test = bert[train], bert[test]\n","\n","    print('Build model '+str(count)+'...')\n","    count += 1\n","\n","    # -------\n","    text_input = Input(shape=(Padding_Length, ), name='text_input')\n","    if model_name in [\"our+overall\", \"our+overall_mot\", \"our+sent+overall\", \"our+sent+overall_mot\", \"our+all\", \"our+all_mot\"]:\n","        overall_feature_input = Input(shape=(num_feature,), name='overall_feature_input')\n","    if model_name in [\"our+sent\", \"our+sent_mot\", \"our+sent+overall\", \"our+sent+overall_mot\", \"our+all\"\"our+all_mot\"]:\n","        sentence_feature_input = Input(shape=(max_sent_length, num_sent_feature,), name='sentence_feature_input')\n","    if model_name in [\"our+bert\", \"our+bert_mot\", \"our+all\", \"our+all_mot\"]:\n","        bert_input = Input(shape=(max_seq_length,), name='bert_input')\n","\n","    embedding_layer = Embedding(input_dim=len(word_index) + 1,\n","                                output_dim=50,\n","                                weights=[embeddings_matrix],\n","                                input_length=Padding_Length,\n","                                trainable=True,\n","                                mask_zero=True)\n","    embedding = embedding_layer(text_input)\n","    if model_name == \"our\":\n","        lstm1 = LSTM(300, dropout=0.5, recurrent_dropout=0.1, return_sequences=True)(embedding)\n","\n","        # 将每个句子的最后输出重新组合\n","        a = [i for i in range(max_word_length - 1, max_word_length * max_sent_length, max_word_length)]\n","        sent_layer = [Lambda(lambda t: t[:, i, :])(lstm1) for i in a]\n","        sent_output = Lambda(stack)(sent_layer)\n","\n","        lstm2 = LSTM(300, dropout=0.5, recurrent_dropout=0.1, return_sequences=False)(sent_output)\n","        dropout = layers.Dropout(0.5)(lstm2)\n","        output = Dense(1, activation='sigmoid')(dropout)\n","        model = Model(inputs=text_input, outputs=output)\n","\n","    elif model_name == \"our_mot\":\n","        lstm1 = LSTM(300, dropout=0.5, recurrent_dropout=0.1, return_sequences=True)(embedding)\n","\n","        # 将每个句子的最后输出重新组合\n","        a = [i for i in range(max_word_length - 1, max_word_length * max_sent_length, max_word_length)]\n","        sent_layer = [Lambda(lambda t: t[:, i, :])(lstm1) for i in a]\n","        sent_output = Lambda(stack)(sent_layer)\n","\n","        lstm2 = LSTM(300, dropout=0.5, recurrent_dropout=0.1, return_sequences=True)(sent_output)\n","        mot = TemporalMean()(lstm2)\n","        dropout = layers.Dropout(0.5)(mot)\n","        output = Dense(1, activation='sigmoid')(dropout)\n","        model = Model(inputs=text_input, outputs=output)\n","\n","    elif model_name == \"our+sent\":\n","        lstm1 = LSTM(300, dropout=0.5, recurrent_dropout=0.1, return_sequences=True)(embedding)\n","        # 将每个句子的最后输出重新组合\n","        a = [i for i in range(max_word_length - 1, max_word_length * max_sent_length, max_word_length)]\n","        sent_layer = [Lambda(lambda t: t[:, i, :])(lstm1) for i in a]\n","        sent_output = Lambda(stack)(sent_layer)\n","        # 每个句子最后的输出结合句子的特征值\n","        sent_feature_concat_layer = Lambda(concatenate)([sent_output, sentence_feature_input])\n","        lstm2 = LSTM(300, dropout=0.5, recurrent_dropout=0.1, return_sequences=False)(sent_feature_concat_layer)\n","        dropout = layers.Dropout(0.5)(lstm2)\n","        output = Dense(1, activation='sigmoid')(dropout)\n","        model = Model(inputs=[text_input, sentence_feature_input], outputs=output)\n","\n","    elif model_name == \"our+sent_mot\":\n","        lstm1 = LSTM(300, dropout=0.5, recurrent_dropout=0.1, return_sequences=True)(embedding)\n","        # 将每个句子的最后输出重新组合\n","        a = [i for i in range(max_word_length - 1, max_word_length * max_sent_length, max_word_length)]\n","        sent_layer = [Lambda(lambda t: t[:, i, :])(lstm1) for i in a]\n","        sent_output = Lambda(stack)(sent_layer)\n","        # 每个句子最后的输出结合句子的特征值\n","        sent_feature_concat_layer = Lambda(concatenate)([sent_output, sentence_feature_input])\n","        lstm2 = LSTM(300, dropout=0.5, recurrent_dropout=0.1, return_sequences=True)(sent_feature_concat_layer)\n","        mot = TemporalMean()(lstm2)\n","        dropout = layers.Dropout(0.5)(mot)\n","        output = Dense(1, activation='sigmoid')(dropout)\n","        model = Model(inputs=[text_input, sentence_feature_input], outputs=output)\n","\n","    elif model_name == \"our+overall\":\n","        lstm1 = LSTM(300, dropout=0.5, recurrent_dropout=0.1, return_sequences=True)(embedding)\n","\n","        # 将每个句子的最后输出重新组合\n","        a = [i for i in range(max_word_length - 1, max_word_length * max_sent_length, max_word_length)]\n","        sent_layer = [Lambda(lambda t: t[:, i, :])(lstm1) for i in a]\n","        sent_output = Lambda(stack)(sent_layer)\n","\n","        lstm2 = LSTM(300, dropout=0.5, recurrent_dropout=0.1, return_sequences=False)(sent_output)\n","        dropout = layers.Dropout(0.5)(lstm2)\n","\n","        # LSTM层输出结合全体特征和BERT\n","        concat_layer = Lambda(concatenate)([dropout, overall_feature_input])\n","\n","        output = Dense(1, activation='sigmoid')(concat_layer)\n","        model = Model(inputs=[text_input, overall_feature_input], outputs=output)\n","\n","    elif model_name == \"our+overall_mot\":\n","        lstm1 = LSTM(300, dropout=0.5, recurrent_dropout=0.1, return_sequences=True)(embedding)\n","\n","        # 将每个句子的最后输出重新组合\n","        a = [i for i in range(max_word_length - 1, max_word_length * max_sent_length, max_word_length)]\n","        sent_layer = [Lambda(lambda t: t[:, i, :])(lstm1) for i in a]\n","        sent_output = Lambda(stack)(sent_layer)\n","\n","        lstm2 = LSTM(300, dropout=0.5, recurrent_dropout=0.1, return_sequences=True)(sent_output)\n","        mot = TemporalMean()(lstm2)\n","        dropout = layers.Dropout(0.5)(mot)\n","\n","        # LSTM层输出结合全体特征和BERT\n","        concat_layer = Lambda(concatenate)([dropout, overall_feature_input])\n","\n","        output = Dense(1, activation='sigmoid')(concat_layer)\n","        model = Model(inputs=[text_input, overall_feature_input], outputs=output)\n","\n","    elif model_name == \"our+bert\":\n","        lstm1 = LSTM(300, dropout=0.5, recurrent_dropout=0.1, return_sequences=True)(embedding)\n","\n","        # 将每个句子的最后输出重新组合\n","        a = [i for i in range(max_word_length - 1, max_word_length * max_sent_length, max_word_length)]\n","        sent_layer = [Lambda(lambda t: t[:, i, :])(lstm1) for i in a]\n","        sent_output = Lambda(stack)(sent_layer)\n","\n","        lstm2 = LSTM(300, dropout=0.5, recurrent_dropout=0.1, return_sequences=False)(sent_output)\n","        dropout = layers.Dropout(0.5)(lstm2)\n","\n","        # LSTM层输出结合全体特征和BERT\n","        concat_layer = Lambda(concatenate)([dropout, bert_input])\n","\n","        output = Dense(1, activation='sigmoid')(concat_layer)\n","        model = Model(inputs=[text_input, bert_input], outputs=output)\n","\n","    elif model_name == \"our+bert_mot\":\n","        lstm1 = LSTM(300, dropout=0.5, recurrent_dropout=0.1, return_sequences=True)(embedding)\n","\n","        # 将每个句子的最后输出重新组合\n","        a = [i for i in range(max_word_length - 1, max_word_length * max_sent_length, max_word_length)]\n","        sent_layer = [Lambda(lambda t: t[:, i, :])(lstm1) for i in a]\n","        sent_output = Lambda(stack)(sent_layer)\n","\n","        lstm2 = LSTM(300, dropout=0.5, recurrent_dropout=0.1, return_sequences=True)(sent_output)\n","        mot = TemporalMean()(lstm2)\n","        dropout = layers.Dropout(0.5)(mot)\n","\n","        # LSTM层输出结合全体特征和BERT\n","        concat_layer = Lambda(concatenate)([dropout, bert_input])\n","\n","        output = Dense(1, activation='sigmoid')(concat_layer)\n","        model = Model(inputs=[text_input, bert_input], outputs=output)\n","\n","    elif model_name == \"our+sent+overall\":\n","        lstm1 = LSTM(300, dropout=0.5, recurrent_dropout=0.1, return_sequences=True)(embedding)\n","\n","        a = [i for i in range(max_word_length - 1, max_word_length * max_sent_length, max_word_length)]\n","        sent_layer = [Lambda(lambda t: t[:, i, :])(lstm1) for i in a]\n","        sent_output = Lambda(stack)(sent_layer)\n","\n","        sent_feature_concat_layer = Lambda(concatenate)([sent_output, sentence_feature_input])\n","        lstm2 = LSTM(300, dropout=0.5, recurrent_dropout=0.1, return_sequences=False)(sent_feature_concat_layer)\n","        dropout = layers.Dropout(0.5)(lstm2)\n","        concat_layer = Lambda(concatenate)([dropout, overall_feature_input])\n","        output = Dense(1, activation='sigmoid')(concat_layer)\n","        model = Model(inputs=[text_input, overall_feature_input, sentence_feature_input], outputs=output)\n","\n","    elif model_name == \"our+sent+overall_mot\":\n","        lstm1 = LSTM(300, dropout=0.5, recurrent_dropout=0.1, return_sequences=True)(embedding)\n","\n","        a = [i for i in range(max_word_length - 1, max_word_length * max_sent_length, max_word_length)]\n","        sent_layer = [Lambda(lambda t: t[:, i, :])(lstm1) for i in a]\n","        sent_output = Lambda(stack)(sent_layer)\n","\n","        sent_feature_concat_layer = Lambda(concatenate)([sent_output, sentence_feature_input])\n","        lstm2 = LSTM(300, dropout=0.5, recurrent_dropout=0.1, return_sequences=True)(sent_feature_concat_layer)\n","        mot = TemporalMean()(lstm2)\n","        dropout = layers.Dropout(0.5)(mot)\n","        concat_layer = Lambda(concatenate)([dropout, overall_feature_input])\n","        output = Dense(1, activation='sigmoid')(concat_layer)\n","        model = Model(inputs=[text_input, overall_feature_input, sentence_feature_input], outputs=output)\n","\n","    elif model_name == \"our+all\":\n","        lstm1 = LSTM(300, dropout=0.5, recurrent_dropout=0.1, return_sequences=True)(embedding)\n","\n","        a = [i for i in range(max_word_length - 1, max_word_length * max_sent_length, max_word_length)]\n","        sent_layer = [Lambda(lambda t: t[:, i, :])(lstm1) for i in a]\n","        sent_output = Lambda(stack)(sent_layer)\n","\n","        sent_feature_concat_layer = Lambda(concatenate)([sent_output, sentence_feature_input])\n","        lstm2 = LSTM(300, dropout=0.5, recurrent_dropout=0.1, return_sequences=False)(sent_feature_concat_layer)\n","        dropout = layers.Dropout(0.5)(lstm2)\n","        concat_layer = Lambda(concatenate)([dropout, overall_feature_input, bert_input])\n","        output = Dense(1, activation='sigmoid')(concat_layer)\n","\n","        model = Model(inputs=[text_input, overall_feature_input, sentence_feature_input, bert_input], outputs=output)\n","\n","    elif model_name == \"our+all_mot\":\n","        lstm1 = LSTM(300, dropout=0.5, recurrent_dropout=0.1, return_sequences=True)(embedding)\n","\n","        a = [i for i in range(max_word_length - 1, max_word_length * max_sent_length, max_word_length)]\n","        sent_layer = [Lambda(lambda t: t[:, i, :])(lstm1) for i in a]\n","        sent_output = Lambda(stack)(sent_layer)\n","\n","        sent_feature_concat_layer = Lambda(concatenate)([sent_output, sentence_feature_input])\n","        lstm2 = LSTM(300, dropout=0.5, recurrent_dropout=0.1, return_sequences=True)(sent_feature_concat_layer)\n","        mot = TemporalMean()(lstm2)\n","        dropout = layers.Dropout(0.5)(mot)\n","        concat_layer = Lambda(concatenate)([dropout, overall_feature_input])\n","        output = Dense(1, activation='sigmoid')(concat_layer)\n","        model = Model(inputs=[text_input, overall_feature_input, sentence_feature_input, bert_input], outputs=output)\n","\n","    # -------\n","\n","    model.summary()\n","\n","    model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.001, decay=1e-6), metrics=['mean_absolute_error'])\n","\n","    if model_name in [\"our\", \"our_mot\"]:\n","        model.fit(text_train, label_train, batch_size=32, epochs=50, verbose=2)\n","    elif model_name in [\"our+sent\", \"our+sent_mot\"]:\n","        model.fit([text_train, sent_train], label_train, batch_size=32, epochs=50, verbose=2)\n","    elif model_name in [\"our+overall\", \"our+overall_mot\"]:\n","        model.fit([text_train, overall_train], label_train, batch_size=32, epochs=50, verbose=2)\n","    elif model_name in [\"our+bert\", \"our+bert_mot\"]:\n","        model.fit([text_train, bert_train], label_train, batch_size=32, epochs=50, verbose=2)\n","    elif model_name in [\"our+sent+overall\", \"our+sent+overall_mot\"]:\n","        model.fit([text_train, overall_train, sent_train], label_train, batch_size=32, epochs=50, verbose=2)\n","    elif model_name in [\"our+all\", \"our+all_mot\"]:\n","        model.fit([text_train, overall_train, sent_train, bert_train], label_train, batch_size=32, epochs=50, verbose=2)\n","\n","    if model_name in [\"our\", \"our_mot\"]:\n","        score_predict = model.predict(text_test)\n","    elif model_name in [\"our+sent\", \"our+sent_mot\"]:\n","        score_predict = model.predict([text_test, sent_test])\n","    elif model_name in [\"our+overall\", \"our+overall_mot\"]:\n","        score_predict = model.predict([text_test, overall_test])\n","    elif model_name in [\"our+bert\", \"our+bert_mot\"]:\n","        score_predict = model.predict([text_test, bert_test])\n","    elif model_name in [\"our+sent+overall\", \"our+sent+overall_mot\"]:\n","        score_predict = model.predict([text_test, overall_test, sent_test])\n","    elif model_name in [\"our+all\", \"our+all_mot\"]:\n","        score_predict = model.predict([text_test, overall_test, sent_test, bert_test])\n","\n","    if prompt_id == 1:\n","        score_predict = score_predict*10+2\n","    elif prompt_id == 2:\n","        score_predict = score_predict*5+1\n","    elif prompt_id == 3 or prompt_id == 4:\n","        score_predict = score_predict*3\n","    elif prompt_id == 5 or prompt_id == 6:\n","        score_predict = score_predict*4\n","    elif prompt_id == 7:\n","        score_predict = score_predict*30\n","    elif prompt_id == 8:\n","        score_predict = score_predict*60\n","\n","    score_predict = np.around(score_predict)\n","\n","    sssscore.append(score_predict)\n","    llllabel.append(score_test)\n","\n","    result = cohen_kappa_score(score_test.values, score_predict, weights='quadratic')\n","    print(\"Kappa Score: {}\".format(result))\n","    results.append(result)\n","    for i in results:\n","      if i != 0:\n","        newresults.append(i)\n","\n","print(\"Average Kappa score after a 5-fold cross validation: \", np.around(np.array(newresults).mean(), decimals=5))\n","\n","sssscore = np.array(sssscore)\n","res = []\n","for scores in sssscore:\n","  for score in scores:\n","    res.append(score)\n","\n","ll = np.array(llllabel)\n","res_label = []\n","for labels in ll:\n","  for label in labels:\n","    res_label.append(label)\n","res_label = np.array(res_label)\n","\n","QWK = cohen_kappa_score(res_label, res, weights='quadratic')\n","print(\"Kappa score after a 5-fold cross validation: \", QWK)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["create_dict\n","Build model 0...\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3239: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","text_input (InputLayer)         (None, 2405)         0                                            \n","__________________________________________________________________________________________________\n","embedding_1 (Embedding)         (None, 2405, 50)     595750      text_input[0][0]                 \n","__________________________________________________________________________________________________\n","lstm_1 (LSTM)                   (None, 2405, 300)    421200      embedding_1[0][0]                \n","__________________________________________________________________________________________________\n","lambda_1 (Lambda)               (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_2 (Lambda)               (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_3 (Lambda)               (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_4 (Lambda)               (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_5 (Lambda)               (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_6 (Lambda)               (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_7 (Lambda)               (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_8 (Lambda)               (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_9 (Lambda)               (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_10 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_11 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_12 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_13 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_14 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_15 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_16 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_17 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_18 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_19 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_20 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_21 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_22 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_23 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_24 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_25 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_26 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_27 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_28 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_29 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_30 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_31 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_32 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_33 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_34 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_35 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_36 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_37 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_38 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_39 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_40 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_41 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_42 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_43 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_44 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_45 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_46 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_47 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_48 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_49 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_50 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_51 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_52 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_53 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_54 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_55 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_56 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_57 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_58 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_59 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_60 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_61 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_62 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_63 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_64 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_65 (Lambda)              (None, 300)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lambda_66 (Lambda)              (None, 65, 300)      0           lambda_1[0][0]                   \n","                                                                 lambda_2[0][0]                   \n","                                                                 lambda_3[0][0]                   \n","                                                                 lambda_4[0][0]                   \n","                                                                 lambda_5[0][0]                   \n","                                                                 lambda_6[0][0]                   \n","                                                                 lambda_7[0][0]                   \n","                                                                 lambda_8[0][0]                   \n","                                                                 lambda_9[0][0]                   \n","                                                                 lambda_10[0][0]                  \n","                                                                 lambda_11[0][0]                  \n","                                                                 lambda_12[0][0]                  \n","                                                                 lambda_13[0][0]                  \n","                                                                 lambda_14[0][0]                  \n","                                                                 lambda_15[0][0]                  \n","                                                                 lambda_16[0][0]                  \n","                                                                 lambda_17[0][0]                  \n","                                                                 lambda_18[0][0]                  \n","                                                                 lambda_19[0][0]                  \n","                                                                 lambda_20[0][0]                  \n","                                                                 lambda_21[0][0]                  \n","                                                                 lambda_22[0][0]                  \n","                                                                 lambda_23[0][0]                  \n","                                                                 lambda_24[0][0]                  \n","                                                                 lambda_25[0][0]                  \n","                                                                 lambda_26[0][0]                  \n","                                                                 lambda_27[0][0]                  \n","                                                                 lambda_28[0][0]                  \n","                                                                 lambda_29[0][0]                  \n","                                                                 lambda_30[0][0]                  \n","                                                                 lambda_31[0][0]                  \n","                                                                 lambda_32[0][0]                  \n","                                                                 lambda_33[0][0]                  \n","                                                                 lambda_34[0][0]                  \n","                                                                 lambda_35[0][0]                  \n","                                                                 lambda_36[0][0]                  \n","                                                                 lambda_37[0][0]                  \n","                                                                 lambda_38[0][0]                  \n","                                                                 lambda_39[0][0]                  \n","                                                                 lambda_40[0][0]                  \n","                                                                 lambda_41[0][0]                  \n","                                                                 lambda_42[0][0]                  \n","                                                                 lambda_43[0][0]                  \n","                                                                 lambda_44[0][0]                  \n","                                                                 lambda_45[0][0]                  \n","                                                                 lambda_46[0][0]                  \n","                                                                 lambda_47[0][0]                  \n","                                                                 lambda_48[0][0]                  \n","                                                                 lambda_49[0][0]                  \n","                                                                 lambda_50[0][0]                  \n","                                                                 lambda_51[0][0]                  \n","                                                                 lambda_52[0][0]                  \n","                                                                 lambda_53[0][0]                  \n","                                                                 lambda_54[0][0]                  \n","                                                                 lambda_55[0][0]                  \n","                                                                 lambda_56[0][0]                  \n","                                                                 lambda_57[0][0]                  \n","                                                                 lambda_58[0][0]                  \n","                                                                 lambda_59[0][0]                  \n","                                                                 lambda_60[0][0]                  \n","                                                                 lambda_61[0][0]                  \n","                                                                 lambda_62[0][0]                  \n","                                                                 lambda_63[0][0]                  \n","                                                                 lambda_64[0][0]                  \n","                                                                 lambda_65[0][0]                  \n","__________________________________________________________________________________________________\n","sentence_feature_input (InputLa (None, 65, 21)       0                                            \n","__________________________________________________________________________________________________\n","lambda_67 (Lambda)              (None, 65, 321)      0           lambda_66[0][0]                  \n","                                                                 sentence_feature_input[0][0]     \n","__________________________________________________________________________________________________\n","lstm_2 (LSTM)                   (None, 300)          746400      lambda_67[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 300)          0           lstm_2[0][0]                     \n","__________________________________________________________________________________________________\n","overall_feature_input (InputLay (None, 25)           0                                            \n","__________________________________________________________________________________________________\n","lambda_68 (Lambda)              (None, 325)          0           dropout_1[0][0]                  \n","                                                                 overall_feature_input[0][0]      \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 1)            326         lambda_68[0][0]                  \n","==================================================================================================\n","Total params: 1,763,676\n","Trainable params: 1,763,676\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","Epoch 1/50\n"," - 331s - loss: 0.0139 - mean_absolute_error: 0.0933\n","Epoch 2/50\n"," - 260s - loss: 0.0125 - mean_absolute_error: 0.0870\n","Epoch 3/50\n"," - 271s - loss: 0.0094 - mean_absolute_error: 0.0777\n","Epoch 4/50\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-50af84332ff5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"our+sent+overall\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"our+sent+overall_mot\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverall_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"our+all\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"our+all_mot\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverall_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}